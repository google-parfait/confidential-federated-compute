# Minimum container that supports llama.cpp with CUDA

This container is a reference container for exploration or testing.
